{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8a7c07b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'merchant_intelligence' from '/Users/loganflecke/Library/CloudStorage/GoogleDrive-logan.flecke@gmail.com/My Drive/GitHub/Finances/merchant_intelligence.py'>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "import merchant_intelligence\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import importlib\n",
    "importlib.reload(merchant_intelligence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1441b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = \"%Y-%m-%d\"\n",
    "local_merch_intel_filename = \"local_merch_intel.csv\"\n",
    "local_category_mapping_filename = \"local_category_mapping.json\"\n",
    "transaction_path = \"./Transactions\"\n",
    "banking_path = \"./Banking\"\n",
    "coc_path = \"./COC\"\n",
    "hysa_path = \"./HYSA\"\n",
    "excel_filename = \"budget.xlsx\"\n",
    "merchant_intelligence_filename = \"merchant_intelligence.csv\"\n",
    "excel = True\n",
    "merch_intel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b5df02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of months to collect, parse, and analyze financial data\n",
    "lookback_months = 8\n",
    "\n",
    "###### CSV Header Names ######\n",
    "## Global Columns, or one that should be normalized to be global, such as date and cost\n",
    "category = 'Category'\n",
    "description = 'Description'\n",
    "date = 'Date'\n",
    "cost = 'Amount'\n",
    "\n",
    "## Credit Card CSV Columns\n",
    "transaction_date = 'Transaction Date'\n",
    "transaction_cost = 'Debit'\n",
    "\n",
    "## Bank Account CSV Columns\n",
    "banking_date = \"Date\"\n",
    "banking_cost = \"Amount\"\n",
    "\n",
    "## Capital One 360 Account CSV Columns\n",
    "co_360_date = 'Transaction Date'\n",
    "co_360_retailer = 'Transaction Description'\n",
    "co_360_cost = 'Transaction Amount'\n",
    "co_360_balance = 'Balance'\n",
    "\n",
    "## THESE ARE THE NAMES OF GROCERIES AS THEY APPEAR ON THE TRANSACTIONS CSV FILE\n",
    "grocery_keywords = ['KROGER', 'GIANT', 'SAFEWAY', 'HELLOFRESH', 'WEGMANS', 'FOOD LION']\n",
    "\n",
    "###### Mappings and Lookups ######\n",
    "with open(local_category_mapping_filename, 'r') as file:\n",
    "    local_category_mapping = json.load(file)\n",
    "\n",
    "transaction_to_expenses_lookup = {\n",
    "    \"Gas/Automotive\" : \"Gas\",\n",
    "    \"Health Care\" : \"Healthcare\",\n",
    "    \"Entertainment\" : \"Other\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f5c09ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MonthlyModel:\n",
    "    month: str\n",
    "    net: str\n",
    "    income: float\n",
    "    expenses: float\n",
    "    fixed_expenses: float\n",
    "    variable_expenses: float\n",
    "    discretionary_expenses: float\n",
    "    investments: float\n",
    "    hy_savings: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "27e95de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_df_by_date(df, date_field):\n",
    "    df[date_field] = pd.to_datetime(df[date_field], format='mixed')\n",
    "    df = df.sort_values(by=date_field, ascending=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[date_field] = df[date_field].dt.strftime('%Y-%m-%d')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a3567026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_events(input_file_path, negate_cost):\n",
    "    # Ingest CSV lines\n",
    "    input_events = [os.path.join(input_file_path, f) for f in os.listdir(input_file_path) if os.path.isfile(os.path.join(input_file_path, f))]\n",
    "    input_df = [pd.read_csv(file) for file in input_events]\n",
    "    merged_events = pd.concat(input_df)\n",
    "    # Normalize column names\n",
    "    column_field_mapping = {\n",
    "        cost : [banking_cost, transaction_cost, co_360_cost],\n",
    "        description : [co_360_retailer],\n",
    "        date : [banking_date, transaction_date, co_360_date]\n",
    "    }\n",
    "    for col in column_field_mapping:\n",
    "        for field in column_field_mapping[col]:\n",
    "            if field in merged_events.columns:\n",
    "                merged_events = merged_events.rename(columns={field: col})\n",
    "                break\n",
    "    # Dedup\n",
    "    merged_events = merged_events.drop_duplicates(subset=[cost, description, date])\n",
    "    # Remove $0 events\n",
    "    merged_events = merged_events[merged_events[cost].notna()]\n",
    "    \n",
    "    # Fill in categories\n",
    "    if not category in merged_events.columns:\n",
    "        # Fill in category for Capital One\n",
    "        if \"COC\" in input_file_path or \"HYSA\" in input_file_path:\n",
    "            merged_events[category] = str(input_file_path).replace(\"./\", \"\")\n",
    "        # Populate the 'category' column based on description\n",
    "        for category_value, descriptions in local_category_mapping.items():\n",
    "            merged_events.loc[merged_events[description].str.contains(\"|\".join(descriptions)), category] = category_value\n",
    "        # Fill in \"Other\" for events not defined in the lookup\n",
    "        merged_events.loc[merged_events[category].isna(), category] = \"Other\"\n",
    "        merged_events = sort_df_by_date(merged_events, date)\n",
    "    if negate_cost:\n",
    "        merged_events = merged_events.assign(**{cost: -merged_events[cost]})\n",
    "    return merged_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cafb3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events_by_date(start_date, end_date, merged_events):\n",
    "    filtered_events = merged_events[(pd.to_datetime(merged_events[date], format=date_format) >= start_date) & \n",
    "                        (pd.to_datetime(merged_events[date], format=date_format) <= end_date)].sort_values(by=date, ascending=True) \n",
    "    return filtered_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4b4f925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_grocery(merged_transactions):\n",
    "    for keyword in grocery_keywords:\n",
    "        contains_keyword = merged_transactions[description].str.contains(keyword, case=False, na=False)\n",
    "        not_fuel = ~merged_transactions[description].str.contains('FUEL', case=False, na=False)\n",
    "        if contains_keyword.any() and not_fuel.any():\n",
    "            merged_transactions.loc[contains_keyword & not_fuel, category] = 'Grocery'\n",
    "    return merged_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "08515978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns normalized event df of all events\n",
    "# Input a dict of dfs (key: df name; value: df; negate_cost_for={df name} for dfs where expenses are positive) \n",
    "def merge_cash_flow(dfs):\n",
    "    frames = []\n",
    "    for name, df in dfs.items():\n",
    "        frames.append(df[[date, description, category, cost]])\n",
    "\n",
    "    return pd.concat(frames, ignore_index=True).sort_values(by=date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a30c4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return income and expense dfs\n",
    "# Input a df of merged events\n",
    "def get_cash_flow(event_dfs: pd.DataFrame):\n",
    "    exception_categories = [\"HYSA Transfer\", \"Investments\", \"Credit Card\"]\n",
    "    event_dfs = event_dfs[~event_dfs[category].isin(exception_categories)]\n",
    "    income_df, expenses_df = pd.DataFrame(), pd.DataFrame()\n",
    "    income_df = event_dfs[event_dfs[cost] > 0]\n",
    "    expenses_df = event_dfs[event_dfs[cost] < 0]\n",
    "    expenses_df.loc[:, cost] = expenses_df.loc[:, cost].abs()\n",
    "    return income_df, expenses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "28e31748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return fixed, variable, and discretionary expenses from an expenses df\n",
    "# Input expenses df\n",
    "def define_expenses(expenses_df):\n",
    "    # Rent, insurance, internet\n",
    "    fixed_expenses_df = expenses_df[expenses_df[category].isin([\"Rent\", \"Car Insurance\", \"Health Care\", \"Internet\"])]\n",
    "    # Groceries, gas, vet\n",
    "    variable_expenses_df = expenses_df[expenses_df[category].isin([\"Gas/Automotive\", \"Grocery\", \"Professional Services\"])]\n",
    "    # Dining, coffee, entertainment\n",
    "    discretionary_expenses_df = expenses_df[\n",
    "        ~expenses_df[category].isin(fixed_expenses_df[category].unique())\n",
    "        & ~expenses_df[category].isin(variable_expenses_df[category].unique())]\n",
    "    return fixed_expenses_df, variable_expenses_df, discretionary_expenses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "caead77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the MonthlyModel dataclass\n",
    "def build_cashflow_model(month_start, master_events_df, income_df, expenses_df):\n",
    "    fixed_expenses_df, variable_expenses_df, discretionary_expenses_df = define_expenses(expenses_df)\n",
    "    return MonthlyModel(\n",
    "        month = month_start,\n",
    "        net = income_df[cost].sum() - expenses_df[cost].sum(),\n",
    "        income = income_df[cost].sum(),\n",
    "        expenses = expenses_df[cost].sum(),\n",
    "        fixed_expenses = fixed_expenses_df[cost].sum(),\n",
    "        variable_expenses = variable_expenses_df[cost].sum(),\n",
    "        discretionary_expenses = discretionary_expenses_df[cost].sum(),\n",
    "        investments = abs(master_events_df.loc[master_events_df[category] == \"Investments\", cost].sum()),\n",
    "        hy_savings = master_events_df.loc[master_events_df[category] == \"HYSA\", cost].sum()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "023fc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the data processing and analysis for a dict event dfs (key: df name; value: df)\n",
    "# Returns the monthly model\n",
    "# Input a dict of dfs (key: df name; value: df)\n",
    "def get_state(merged_event_dfs, month_start, month_end):\n",
    "    # Get time period (length of unit of time minus 1 day)\n",
    "    filtered_transactions = filter_events_by_date(month_start, month_end, merged_event_dfs[\"transactions\"])\n",
    "    filtered_banking = filter_events_by_date(month_start, month_end, merged_event_dfs[\"banking\"])\n",
    "    filtered_coc = filter_events_by_date(month_start, month_end, merged_event_dfs[\"coc\"])\n",
    "    filtered_hysa = filter_events_by_date(month_start, month_end, merged_event_dfs[\"hysa\"])\n",
    "\n",
    "    master_events_df = merge_cash_flow(\n",
    "        {\n",
    "            \"transactions\": filtered_transactions,\n",
    "            \"banking\": filtered_banking,\n",
    "            \"coc\": filtered_coc,\n",
    "            \"hysa\": filtered_hysa,\n",
    "        }\n",
    "    )\n",
    "    income_df, expenses_df = get_cash_flow(master_events_df)\n",
    "    return build_cashflow_model(month_start, master_events_df, income_df, expenses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "76a23307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a df of X number of monthly models, one per row\n",
    "# Input a dict of dfs (key: df name; value: df)\n",
    "def iterate_months(merged_event_dfs, months_back: int):\n",
    "    states = []\n",
    "\n",
    "    today = datetime.date.today()\n",
    "    current_month_start = today.replace(day=1)\n",
    "\n",
    "    for i in range(months_back):\n",
    "        # shift month back by i\n",
    "        year = current_month_start.year\n",
    "        month = current_month_start.month - i\n",
    "\n",
    "        while month <= 0:\n",
    "            month += 12\n",
    "            year -= 1\n",
    "\n",
    "        month_start = datetime.date(year, month, 1)\n",
    "\n",
    "        # compute month end\n",
    "        next_month = month_start.replace(day=28) + datetime.timedelta(days=4)\n",
    "        month_end = next_month - datetime.timedelta(days=next_month.day)\n",
    "\n",
    "        states.append(\n",
    "            get_state(\n",
    "                merged_event_dfs,\n",
    "                pd.to_datetime(month_start),\n",
    "                pd.to_datetime(month_end),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    state_df = pd.DataFrame(states)\n",
    "    state_df.columns = (col.title() for col in state_df.columns)\n",
    "    return sort_df_by_date(state_df, \"Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "eae26ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_excel(dataframe_sheets):\n",
    "    with pd.ExcelWriter(excel_filename) as writer:\n",
    "        keys_list = list(dataframe_sheets.keys())\n",
    "        for sheet in dataframe_sheets:\n",
    "            dataframe_sheets[sheet].to_excel(writer, sheet_name=sheet, index=keys_list.index(sheet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c8d289e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_expenses(expenses_df):\n",
    "    value_counts = expenses_df[description].value_counts()\n",
    "    expenses_df[date] = pd.to_datetime(expenses_df[date])\n",
    "    cutoff_date = datetime.datetime.now() - pd.DateOffset(months=lookback_months)\n",
    "    expenses_df.loc[:, 'frequency'] = expenses_df[description].map(value_counts).astype(int)\n",
    "    recent_frequented_merchants = expenses_df.loc[(expenses_df['frequency'] > 1) & (expenses_df[date] >= cutoff_date), description].drop_duplicates(ignore_index=True)\n",
    "    return recent_frequented_merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "18dc10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    merged_transactions = merge_events(transaction_path, negate_cost=True)\n",
    "    merged_banking = merge_events(banking_path, negate_cost=False)\n",
    "    merged_coc = merge_events(coc_path, negate_cost=False)\n",
    "    merged_hysa = merge_events(hysa_path, negate_cost=False)\n",
    "    merged_transactions = enrich_grocery(merged_transactions)\n",
    "\n",
    "    merged_event_dfs = {\n",
    "        \"transactions\" : merged_transactions, \n",
    "        \"banking\" : merged_banking, \n",
    "        \"coc\" : merged_coc, \n",
    "        \"hysa\" : merged_hysa\n",
    "    }\n",
    "\n",
    "    date_start = pd.to_datetime(datetime.date.today() - relativedelta(months=lookback_months)).replace(day=1)\n",
    "    date_end = pd.to_datetime(datetime.date.today())\n",
    "\n",
    "    net_df = iterate_months(merged_event_dfs, lookback_months)\n",
    "    \n",
    "    merged_all = merge_cash_flow(merged_event_dfs)\n",
    "    \n",
    "    filtered_all = filter_events_by_date(date_start, date_end, merged_all)\n",
    "    income_df, expenses_df = get_cash_flow(filtered_all)\n",
    "    \n",
    "    if merch_intel:\n",
    "        transaction_intel_df = merchant_intelligence.build_merchant_intel(expenses_df[description], local_merch_intel_filename)\n",
    "        expenses_df = expenses_df.merge(\n",
    "            transaction_intel_df[['domain', 'handle', 'type', 'name', 'founded', 'industry', 'size', 'hq_city', 'hq_state', 'hq_state_code', 'hq_country_code', 'transaction']],\n",
    "            left_on=description,\n",
    "            right_on='transaction',\n",
    "            how='left'\n",
    "        ).drop(columns=['transaction']).drop_duplicates()\n",
    "        print(\"Merchant intelligence successfully executed:\", merchant_intelligence_filename)\n",
    "        print(\"Merchant Intelligence Coverage: \", float(len(transaction_intel_df) / len(expenses_df)).__round__(2))\n",
    "    \n",
    "    fixed_expenses_df, variable_expenses_df, discretionary_expenses_df = define_expenses(expenses_df)\n",
    "\n",
    "    if excel == True:\n",
    "        dataframe_sheets = {\n",
    "            \"Net\" : net_df,\n",
    "            \"Fixed Expenses\" : fixed_expenses_df,\n",
    "            \"Variable Expenses\" : variable_expenses_df,\n",
    "            \"Discretionary Expenses\" : discretionary_expenses_df,\n",
    "            \"All Transactions\" : merged_transactions,\n",
    "            \"All Banking\" : merged_banking,\n",
    "            \"All Cap One Checking\" : merged_coc,\n",
    "            \"All HYSA\" : merged_hysa\n",
    "        }\n",
    "        try:\n",
    "            export_to_excel(dataframe_sheets)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create Excel file:\", e)\n",
    "        else:\n",
    "            print(\"Excel file created successfully.\", excel_filename)\n",
    "    return transaction_intel_df, expenses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "50d893bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant intelligence successfully executed: merchant_intelligence.csv\n",
      "Merchant Intelligence Coverage:  1.04\n",
      "Excel file created successfully. budget.xlsx\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    transaction_intel_df, expenses_df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d4fcbe6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "Smoothie King (SKFI)               10\n",
       "DoorDash                            8\n",
       "Dunkin'                             7\n",
       "Toast                               6\n",
       "Subway                              5\n",
       "Panera Bread                        4\n",
       "The Wendy's Company                 3\n",
       "Swingers - the crazy golf club      2\n",
       "Salsarita's Fresh Mexican Grill     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expenses_df.loc[expenses_df[category] == \"Dining\", 'name'].value_counts().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "41740ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction_intel_df.to_csv(\"transaction_intel_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3ee3274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction_intel_df[['domain', 'name']].drop_duplicates().to_csv(\"unique_companies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f8dd20d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transaction_intel_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "45371546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expenses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6243f1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0420353982300885"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transaction_intel_df) / len(expenses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1df361cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_start = pd.to_datetime(datetime.date.today() - relativedelta(months=8)).replace(day=1)\n",
    "# date_end = pd.to_datetime(datetime.date.today())\n",
    "\n",
    "# merged_transactions = merge_events(transaction_path, negate_cost=True)\n",
    "# merged_banking = merge_events(banking_path, negate_cost=False)\n",
    "# merged_coc = merge_events(coc_path, negate_cost=False)\n",
    "# merged_hysa = merge_events(hysa_path, negate_cost=False)\n",
    "# merged_transactions = enrich_grocery(merged_transactions)\n",
    "\n",
    "# merged_event_dfs = {\n",
    "#     \"transactions\" : merged_transactions, \n",
    "#     \"banking\" : merged_banking, \n",
    "#     \"coc\" : merged_coc, \n",
    "#     \"hysa\" : merged_hysa\n",
    "# }\n",
    "\n",
    "# merged_all = merge_cash_flow(merged_event_dfs)\n",
    "\n",
    "# filtered_all = filter_events_by_date(date_start, date_end, merged_all)\n",
    "# income_df, expenses_df = get_cash_flow(filtered_all)\n",
    "\n",
    "# transaction_intel_df = merchant_intelligence.build_merchant_intel(expenses_df[description], local_merch_intel_filename)\n",
    "# expenses_df = expenses_df.merge(\n",
    "#     transaction_intel_df,\n",
    "#     left_on=description,\n",
    "#     right_on='transaction',\n",
    "#     how='left'\n",
    "# ).drop(columns=['transaction']).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "953320be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merch_intel_cols = ['domain', 'categories', 'handle', 'type', 'name', 'founded', 'industry', 'size', 'hq_city', 'hq_state', 'hq_state_code', 'hq_country_code', 'summary', 'transaction']\n",
    "# local_merch_intel_df = pd.read_csv(local_merch_intel_filename, usecols=merch_intel_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "69098055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_transactions = merge_events(transaction_path, negate_cost=True)\n",
    "# merged_banking = merge_events(banking_path, negate_cost=False)\n",
    "# merged_coc = merge_events(coc_path, negate_cost=False)\n",
    "# merged_hysa = merge_events(hysa_path, negate_cost=False)\n",
    "# merged_transactions = enrich_grocery(merged_transactions)\n",
    "\n",
    "# merged_event_dfs = {\n",
    "#     \"transactions\" : merged_transactions, \n",
    "#     \"banking\" : merged_banking, \n",
    "#     \"coc\" : merged_coc, \n",
    "#     \"hysa\" : merged_hysa\n",
    "# }\n",
    "\n",
    "# merged_all = merge_cash_flow(merged_event_dfs)\n",
    "\n",
    "# date_start = pd.to_datetime(datetime.date.today() - relativedelta(months=lookback_months)).replace(day=1)\n",
    "# date_end = pd.to_datetime(datetime.date.today())\n",
    "\n",
    "# filtered_all = filter_events_by_date(date_start, date_end, merged_all)\n",
    "# income_df, expenses_df = get_cash_flow(filtered_all)\n",
    "\n",
    "# transaction_intel_df = merchant_intelligence.build_merchant_intel(expenses_df[description], local_merch_intel_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "66d4b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(local_merch_intel_df.loc[local_merch_intel_df['transaction'].isin(expenses_df[description])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
